{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will learn how to apply Reinforcement Learning to train an agent to play Super Mario Bros. We will build up the entire project step by step and also give yourself a chance to explore the packages and functions behind it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First, let's import our packages that we need. We will be using the `gym_super_mario_bros` package which is an OpenAI Gym enviroment for Super Mario Bros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY,SIMPLE_MOVEMENT,COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "from gym import Wrapper\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
    "import os\n",
    "from PIL import Image\n",
    "from wrappers import *\n",
    "from agent import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions\n",
    "The Super Mario Gym API gives us a lot of functions that we use right of the bat. These are the exact same functions that OpenAI Gym uses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what actions we can take with Mario. According to the documenation, \"gym_super_mario_bros.actions provides three actions lists (RIGHT_ONLY, SIMPLE_MOVEMENT, and COMPLEX_MOVEMENT) \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The actions in RIGHT_ONLY are: \", RIGHT_ONLY)\n",
    "print(\"The actions in SIMPLE_MOVEMENT are: \", SIMPLE_MOVEMENT)\n",
    "print(\"The actions in COMPLEX_MOVEMENT are: \", COMPLEX_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're familiar with the NES controller, you might recognize some of these actions. For example, the `right` and `A` key performs a jump while moving right. I'm sure you get the idea. Another note: The `\"NOOP\"` means no operation, or in this case, not do anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](NES.jpg \"NES Controller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define our enviroment. We will use the variable `ENV_NAME` to store the string corresponding to the level of the game. According to the documentation, we specify the enviroment in the form \n",
    "```python\n",
    "SuperMarioBros-<world>-<stage>-v<version>\n",
    "```\n",
    "where:\n",
    "\n",
    "- `<world>` is a number in {1, 2, 3, 4, 5, 6, 7, 8} indicating the world\n",
    "- `<stage>` is a number in {1, 2, 3, 4} indicating the stage within a world\n",
    "- `<version>` is a number in {0, 1, 2, 3} specifying the ROM mode to use\n",
    "    - 0: standard ROM\n",
    "    - 1: downsampled ROM\n",
    "    - 2: pixel ROM\n",
    "    - 3: rectangle ROM\n",
    "\n",
    "Thus, if we want to play 3-1 on the standard ROM, you would use the environment id `SuperMarioBros-3-1-v0`. We will be basic and play 1-1 on the standard ROM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"SuperMarioBros-1-1-v0\"\n",
    "env = gym_super_mario_bros.make(ENV_NAME, render_mode = 'human', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, RIGHT_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with any gym enviroment, we call the `.make()` function and pass in our enviroment name, along with specifiying some parameters.\n",
    "- `render_mode`: tells the enviroment how it should be rendered. Check documentation.\n",
    "- `apply_api_compatibility`: allows use to use recent versions of OpenAI Gym\n",
    "\n",
    "Then, we will wrap it with the `JoypadSpace` to allow the code to control and play as Mario, passing in our action space (`RIGHT_ONLY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = RIGHT_ONLY.index(['right'])\n",
    "    _,_,done,_,_ = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then make a function which handles applying the wrappers to enviroment. We will apply \n",
    "- SkipFrame: perform the same action for 4 frames\n",
    "- ResizeObservation: resize frame from 240x256 to 84x84\n",
    "- GrayScaleObservation: make the frame greyscale \n",
    "- FrameStack: compresses the frames together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In `wrappers.py`, complete the step function in the SkipFrame class that inherits from the wrapper class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def step(self, action):\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    for _ in range(self.skip):\n",
    "        next_state, reward, done, trunc, info = self.env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return next_state, total_reward, done, trunc, info\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run this cell below to check if your implementation is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MockEnv:\n",
    "    def __init__(self):\n",
    "        self.current_state = 0\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        self.step_count = 0\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Simulate the environment's response to an action\n",
    "        self.current_state += 1\n",
    "        reward = 1  # Fixed reward for simplicity\n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= 10  # Terminal state after 10 steps\n",
    "        trunc = False  # For simplicity, we won't use truncation in this mock\n",
    "        info = {}\n",
    "        return self.current_state, reward, done, trunc, info\n",
    "\n",
    "\n",
    "def test_skip_frame():\n",
    "    env = MockEnv()\n",
    "    skip_env = SkipFrame(env, skip=4)\n",
    "    total_rewards = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        next_state, total_reward, done, trunc, info = skip_env.step(action=0)\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    # Check if the total rewards accumulated are as expected\n",
    "    expected_rewards = [4] * (10 // 4)  # Adjust based on your mock environment and skip value\n",
    "    assert all(tr == er for tr, er in zip(total_rewards, expected_rewards)), \"Test failed :(\"\n",
    "\n",
    "    print(\"Test passed! :D.\")\n",
    "\n",
    "test_skip_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the `apply_wrappers()` function in `wrapper.py` which will execute all of the wrappers above. You just need to add some parameters in to make sure it fits the description!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent NN Class\n",
    "Now, we will implement the agent nn class which will outline the architecture of the CNN. We will implement this with PyTorch. Check this out for a great resource: [Link](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In `agent_nn.py`, within the AgentNN class, implement the CNN architecture. Be sure to include \n",
    "- Convolution Layers\n",
    "- Linear Layers\n",
    "\n",
    "**Make sure to check what the input and outputs for each layer!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented our network architecture, let's implement the agent class which will do the learning (calculate rewards). Try looking through the file and make sure you understand how the class is structured. You will see 2 networks. Online vs target network. Let's see what the difference is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online v Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target and online networks serve different roles in the learning process, helping to stabilize the training of the agent. \n",
    "### Online Network\n",
    "\n",
    "- **Role**: The online network, also known as the policy network, is directly involved in the decision-making process. It evaluates the current state and predicts the Q-values for each action. The action with the highest Q-value (or a random action, depending on the epsilon-greedy strategy) is selected by the agent to perform in the environment.\n",
    "- **Training**: The weights of the online network are updated continuously at every learning step based on the loss calculated between its predicted Q-values and the target Q-values derived from the target network.\n",
    "- **Purpose**: The main purpose of the online network is to learn and improve the policy the agent follows by minimizing the difference between its predicted Q-values and the target Q-values.\n",
    "\n",
    "### Target Network\n",
    "\n",
    "- **Role**: The target network has a similar architecture to the online network but serves a different purpose. It is used to generate the target Q-values for the next state when calculating the loss during training. These target Q-values are used to provide a stable goal for the online network to achieve.\n",
    "- **Training**: The weights of the target network are updated less frequently, often by copying the weights from the online network at regular intervals. This infrequent update schedule helps to stabilize the training process.\n",
    "- **Purpose**: Its primary purpose is to stabilize learning by providing consistent targets for the online network's updates. Without it, the training process can become unstable due to the constantly shifting targets, as the same network would be generating the predictions and also providing the targets for those predictions.\n",
    "\n",
    "### Why the Distinction Matters\n",
    "\n",
    "The use of separate target and online networks addresses the problem of moving targets in Q-learning. In a constantly changing environment, where both the policy and the value estimates are being updated simultaneously, having a stable target for value estimation is crucial. The target network provides this stability, as its parameters are updated less frequently, thereby making the training process more stable and reliable.\n",
    "\n",
    "This separation essentially helps mitigate the risk of positive feedback loops where the network's predictions can become overly optimistic, leading to poor policy decisions. By decoupling the generation of target Q-values from the online network's predictions, it ensures that the agent learns from a slightly out-of-date, but more stable, version of its own value estimates, leading to more robust learning outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen what both networks are used for, implement the `choose_action()` and `learn()` functions in `agent.py`, ensuring that you pass in the correct inputs! If you need a peak at the solutions, feel free to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "predicted_q_values = self.online_network(states)\n",
    "predicted_q_values = predicted_q_values[np.arange(self.batch_size), actions.squeeze()]\n",
    "target_q_values = self.target_network(next_states).max(dim=1)[0]\n",
    "target_q_values = rewards + self.gamma * target_q_values * (1 - dones.float())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have gone through all of the parts that we need to put this all togther. Run `main.py` to train the model. Make sure you look over the code to make sure you understand what the code is doing. Since we are running for 50,000 episodes, this will take a long while, depending on your machine. Try running it for 5000 episodes and the model will save itself in the same folder. Don't worry if you're not able to train it for the whole 50000 episodes. It is a lot. Play around with the code. Some things you can play around with are:\n",
    "\n",
    "### 1. **Learning Rate (`lr`)**:\n",
    "- The rate at which the agent learns from each batch of experiences. Adjusting the learning rate can have a significant impact on the convergence and stability of training.\n",
    "\n",
    "### 2. **Discount Factor (`gamma`)**:\n",
    "- Determines the importance of future rewards. A higher value places more emphasis on future rewards, which can affect the agent's strategy significantly.\n",
    "\n",
    "### 3. **Batch Size**:\n",
    "- The number of experiences sampled from the replay buffer for each learning step. Changing the batch size can affect learning dynamics and computational efficiency.\n",
    "\n",
    "### 4. **Action Space**:\n",
    "- Choosing different sets of actions (e.g., `RIGHT_ONLY` vs. `SIMPLE_MOVEMENT` vs. `COMPLEX_MOVEMENT`) changes the complexity of the decision-making problem and can lead to different behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "Once you are able to train the model to some extent, try exploring with different paramters. Below, I change the learning rate and discount rates and plot it versus rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameters to explore\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "discount_factors = [0.99, 0.95, 0.90]\n",
    "NUM_OF_EPISODES = 500\n",
    "ENV_NAME = 'SuperMarioBros-1-1-v0\n",
    "# Store results\n",
    "experiment_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for gamma in discount_factors:\n",
    "        # Declare your enviroment \n",
    "        env = gym_super_mario_bros.make(ENV_NAME, render_mode='human', apply_api_compatibility=True)\n",
    "        env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "        #Apply the wrappers\n",
    "        env = apply_wrappers(env)\n",
    "\n",
    "        # instantiate an agent of agent class\n",
    "        agent = Agent(input_dims=env.observation_space.shape, num_actions=env.action_space.n, lr=lr, gamma=gamma)\n",
    "        \n",
    "        total_rewards = []\n",
    "        \n",
    "        for episode in range(NUM_OF_EPISODES):\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.choose_action(state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                agent.store_in_memory(state, action, reward, new_state, done)\n",
    "                agent.learn()\n",
    "                state = new_state\n",
    "                total_reward += reward\n",
    "            \n",
    "            total_rewards.append(total_reward)\n",
    "        \n",
    "        # Store the results\n",
    "        experiment_results[(lr, gamma)] = total_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Great job! I hope you learned something through these 10 weeks, albeit it was somewhat not well structured. I hope this showed up what the tip of reinforcement learning is and how it can be applied. Training takes a while but the finished product is pretty cool! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
